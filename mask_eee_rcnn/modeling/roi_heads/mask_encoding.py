import numpy as np
import torch
import torch_dct

import math
from detectron2.structures import Boxes, Instances
from detectron2.layers import cat

def add_ground_truth_to_pred_boxes(gt_boxes, proposals):
    """
    Call `add_ground_truth_to_proposals_single_image` for all images.

    Args:
        gt_boxes(list[Boxes]): list of N elements. Element i is a Boxes
            representing the gound-truth for image i.
        proposals (list[Instances]): list of N elements. Element i is a Instances
            representing the proposals for image i.

    Returns:
        list[Instances]: list of N Instances. Each is the proposals for the image,
            with field "proposal_boxes" and "objectness_logits".
    """
    assert gt_boxes is not None

    assert len(proposals) == len(gt_boxes)
    if len(proposals) == 0:
        return proposals

    return [
        add_ground_truth_to_pred_boxes_single_image(gt_boxes_i, proposals_i)
        for gt_boxes_i, proposals_i in zip(gt_boxes, proposals)
    ]


def add_ground_truth_to_pred_boxes_single_image(gt_boxes, proposals):
    """
    Augment `proposals` with ground-truth boxes from `gt_boxes`.

    Args:
        Same as `add_ground_truth_to_proposals`, but with gt_boxes and proposals
        per image.

    Returns:
        Same as `add_ground_truth_to_proposals`, but for only one image.
    """
    device = proposals.pred_boxes.device
    # Assign all ground-truth boxes an objectness logit corresponding to
    # P(object) = sigmoid(logit) =~ 1.
    gt_logit_value = math.log((1.0 - 1e-10) / (1 - (1.0 - 1e-10)))
    gt_logits = gt_logit_value * torch.ones(len(gt_boxes), device=device)

    # Concatenating gt_boxes with proposals requires them to have the same fields
    gt_proposal = Instances(proposals.image_size)
    gt_proposal.pred_boxes = gt_boxes
    gt_proposal.scores = gt_logits
    new_proposals = Instances.cat([proposals, gt_proposal])

    return new_proposals

def patch2masks(patch, scale, patch_size, mask_size):
    """
    assemble patches to obtain an entire mask
    Args:
        mask_rc: A tensor of shape [B*num_patch,patch_size,patch_size]
        scale: A NxN mask size is divided into scale x scale patches
        patch_size: size of each patch
        mask_size: size of masks generated by PatchDCT

    Returns:
        A tensor of shape [B,mask_size,mask_size].The masks obtain assemble by patches
    """
    patch = patch.reshape(-1, scale, scale, patch_size, patch_size)
    patch = patch.permute(0, 1, 2, 4, 3)
    patch = patch.reshape(-1, scale, mask_size, patch_size)
    patch = patch.permute(0, 1, 3, 2)
    mask = patch.reshape(-1, mask_size, mask_size)
    return mask

def masks2patch(masks_per_image, scale, patch_size, mask_size):
    """

    Args:
        masks_per_image: A tensor of shape [B,mask_size,mask_size]
        scale: A NxN mask size is divided into scale x scale patches
        patch_size: size of each patch
        mask_size: size of masks generated by PatchDCT

    Returns:
        patches_per_image: A tensor of shape [B*num_patch,patch_size,patch_size]. The patches obtained by masks

    """
    masks_per_image = masks_per_image.reshape(-1, scale, patch_size,mask_size)
    masks_per_image = masks_per_image.permute(0, 1, 3, 2)
    masks_per_image = masks_per_image.reshape(-1, scale, scale, patch_size, patch_size)
    masks_per_image = masks_per_image.permute(0, 1, 2, 4, 3)
    patches_per_image = masks_per_image.reshape(-1,patch_size, patch_size)
    return patches_per_image


class DctMaskEncoding(object):
    """
    Apply DCT to encode the binary mask, and use the encoded vector as mask representation in instance segmentation.
    """
    def __init__(self, vec_dim, mask_size=128):
        """
        vec_dim: the dimension of the encoded vector, int
        mask_size: the resolution of the initial binary mask representaiton.
        """
        self.vec_dim = vec_dim
        self.mask_size = mask_size
        assert vec_dim <= mask_size*mask_size
        self.dct_vector_coords = self.get_dct_vector_coords(r=mask_size)

    def encode(self, masks, dim=None):
        """
        Encode the mask to vector of vec_dim or specific dimention.
        """
        if dim is None:
            dct_vector_coords = self.dct_vector_coords[:self.vec_dim]
        else:
            dct_vector_coords = self.dct_vector_coords[:dim]
        masks = masks.view([-1, self.mask_size, self.mask_size]).to(dtype=float)  # [N, H, W]
        dct_all = torch_dct.dct_2d(masks, norm='ortho')
        xs, ys = dct_vector_coords[:, 0], dct_vector_coords[:, 1]
        dct_vectors = dct_all[:, xs, ys]  # reshape as vector
        return dct_vectors  # [N, D]

    def decode(self, dct_vectors, dim=None):
        """
        intput: dct_vector numpy [N,dct_dim]
        output: mask_rc mask reconstructed [N, mask_size, mask_size]
        """
        device = dct_vectors.device
        if dim is None:
            dct_vector_coords = self.dct_vector_coords[:self.vec_dim]
        else:
            dct_vector_coords = self.dct_vector_coords[:dim]
            dct_vectors = dct_vectors[:, :dim]

        N = dct_vectors.shape[0]
        dct_trans = torch.zeros([N, self.mask_size, self.mask_size], dtype=dct_vectors.dtype).to(device)
        xs, ys = dct_vector_coords[:, 0], dct_vector_coords[:, 1]
        dct_trans[:, xs, ys] = dct_vectors
        mask_rc = torch_dct.idct_2d(dct_trans, norm='ortho')  # [N, mask_size, mask_size]
        return mask_rc

    def get_dct_vector_coords(self, r=128):
        """
        Get the coordinates with zigzag order.
        """
        dct_index = []
        for i in range(r):
            if i % 2 == 0:  # start with even number
                index = [(i-j, j) for j in range(i+1)]
                dct_index.extend(index)
            else:
                index = [(j, i-j) for j in range(i+1)]
                dct_index.extend(index)
        for i in range(r, 2*r-1):
            if i % 2 == 0:
                index = [(i-j, j) for j in range(i-r+1,r)]
                dct_index.extend(index)
            else:
                index = [(j, i-j) for j in range(i-r+1,r)]
                dct_index.extend(index)
        dct_idxs = np.asarray(dct_index)
        return dct_idxs


class GT_infomation:
    def __init__(self,mask_size_assemble, mask_size, patch_size, scale, dct_encoding, patch_dct_encoding):
        self.mask_size_assemble = mask_size_assemble
        self.mask_size = mask_size
        self.patch_size = patch_size
        self.scale = scale
        self.dct_encoding = dct_encoding
        self.patch_dct_encoding =  patch_dct_encoding
    
    def get_gt_mask(self, instances, pred_mask_logits, pred_instances=None):
        gt_masks = []
        gt_classes = []
        gt_masks_coarse = []
        for idx, instances_per_image in enumerate(instances):

            if len(instances_per_image) == 0:
                continue

            if pred_instances == None:
                gt_masks_per_image = instances_per_image.gt_masks.crop_and_resize(
                    instances_per_image.proposal_boxes.tensor, self.mask_size)
                gt_masks_coarse.append(gt_masks_per_image)

                gt_masks_per_image = instances_per_image.gt_masks.crop_and_resize(
                    instances_per_image.proposal_boxes.tensor, self.mask_size_assemble)
                # divided masks into scalexscale patch,patch size=8
                gt_masks_per_image = masks2patch(gt_masks_per_image,self.scale,self.patch_size,self.mask_size_assemble)
                gt_masks.append(gt_masks_per_image)
            else:
                gt_masks_per_image = instances_per_image.gt_masks.crop_and_resize(
                    pred_instances[idx].pred_boxes.tensor, self.mask_size)
                gt_masks_coarse.append(gt_masks_per_image)

                gt_masks_per_image = instances_per_image.gt_masks.crop_and_resize(
                    pred_instances[idx].pred_boxes.tensor, self.mask_size_assemble)
                # divided masks into scalexscale patch,patch size=8
                gt_masks_per_image = masks2patch(gt_masks_per_image,self.scale,self.patch_size,self.mask_size_assemble)
                gt_masks.append(gt_masks_per_image)

            gt_classes_per_image = instances_per_image.gt_classes.to(dtype=torch.int64)
            gt_classes.append(gt_classes_per_image)

        if len(gt_masks) == 0:
            return pred_mask_logits.sum() * 0
        gt_masks = cat(gt_masks, dim=0)
        gt_masks = self.patch_dct_encoding.encode(gt_masks)  # [N, dct_v_dim]
        gt_masks = gt_masks.to(dtype=torch.float32) #[N_instance,pdct_vector_dim]
        gt_classes = cat(gt_classes, dim=0) #[N_instanc]
        gt_masks_coarse = cat(gt_masks_coarse,dim=0)
        gt_masks_coarse = self.dct_encoding.encode(gt_masks_coarse).to(dtype=torch.float32)
        gt_masks, gt_bfg = self.get_gt_bfg(gt_masks)
        return gt_masks,gt_classes,gt_masks_coarse,gt_bfg


    def get_gt_mask_inference(self,instances,pred_mask_logits):
        gt_masks = []

        for instances_per_image in instances:
            if len(instances_per_image) == 0:
                continue
            if instances_per_image.has("gt_masks"):
                gt_masks_per_image = instances_per_image.gt_masks.crop_and_resize(
                    instances_per_image.pred_boxes.tensor, self.mask_size_assemble)
            else:
                #print("gt_mask is empty")
                shape = instances_per_image.pred_boxes.tensor.shape[0]
                device = instances_per_image.pred_boxes.tensor.device
                gt_masks_per_image = torch.zeros((shape,self.mask_size_assemble,self.mask_size_assemble),dtype=torch.bool).to(device)

            gt_masks_per_image = masks2patch(gt_masks_per_image,self.scale,self.patch_size,self.mask_size_assemble)
            gt_masks.append((gt_masks_per_image))

        if len(gt_masks) == 0:
            return pred_mask_logits.sum() * 0

        gt_masks = cat(gt_masks, dim=0)
        gt_masks = self.patch_dct_encoding.encode(gt_masks)
        gt_masks = gt_masks.to(dtype=torch.float32)
        gt_masks, gt_bfg = self.get_gt_bfg(gt_masks)
        return gt_masks,gt_bfg

    def get_gt_bfg(self, gt_masks):
        gt_bfg = gt_masks[:, 0].clone()
        gt_bfg[(gt_bfg > 0) & (gt_bfg < self.patch_size)] = 1.
        gt_bfg[gt_bfg == self.patch_size] = 2
        gt_bfg = gt_bfg.to(dtype=torch.int64)
        gt_masks = gt_masks[gt_bfg == 1, :]
        return gt_masks, gt_bfg
    
    def get_gt_classes(self, instances):
        gt_classes = []
        for instances_per_image in instances:
            if len(instances_per_image) == 0:
                continue
            gt_classes_per_image = instances_per_image.gt_classes.to(dtype=torch.int64)
            gt_classes.append(gt_classes_per_image)
        gt_classes = cat(gt_classes, dim=0)  # [N_instance]
        return gt_classes

